\documentclass{article}

% *** LANGUAGE PACKAGES ***
%
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}      
\usepackage[francais]{babel}

% *** LAYOUT PACKAGES ***
%
\usepackage[top=4cm, bottom=4cm, left=4cm, right=4cm]{geometry}

% *** GRAPHICS PACKAGES ***
%
\usepackage{graphicx}


\title{Analyse et traitement de l'information : TP3}
\author{Djavan Sergent}
\date{\today}


\begin{document}
	\maketitle
	\section{Principal Component Analysis (PCA)}
		\subsection{}
		\begin{center}
			\includegraphics[width=400pt]{figure1_1.png}
		\end{center}

		\textbf{Valeurs obtenues :} 50\%  : m = 10  | 95\%  : m = 126 | 100\% : m = 528 \\
		
		Comme on peut le voir, seules 10 composantes permettent de récupérer la moitié de l'image, ce qui signifie que l'information principale est contenue dans les premiers m. Pour obtenir 100\%, le nombre de composantes requises varie légèrement autour de 530 lors des différentes exécutions.
		
		\subsection{}
		Chaque ligne des images reconstruites ci-dessous comporte en premier élément l'image originale suivi des dix images qui représentent le k correspondant, du plus petit au plus grand.
			\begin{center}
				\includegraphics[width=400pt]{figure1_2.png}
				Images reconstruites
			\end{center}
	
		\subsection{}
		Avec les résultats ci-dessus, on peut constater que même en conservant un k petit on distingue déjà clairement la forme de l'image. On constate également que ce k petit laisse une grande part d'insertitude autour de la forme principale malgré qu'on ai pu en reconstituer 50 pourcents avec k=10.
	
	\section{k-Nearest-Neighbour (k-NN) Classification}
	
	\subsection{}
	On obtient une précision de l'ordre de 98 pourcents avec un échantillon de 5000 images. Ces très bons résultats démontrent l'intérêt de la méthode k-NN pour la classification des labels de l'ensemble de test.
	
	\subsection{}
	En combinant les deux approches, on obtient les valeurs suivantes :
	\begin{description}
		\item[m =   10] 0.9587
		\item[m =   20] 0.9800
		\item[m =   30] 0.9738
		\item[m =   40] 0.9712
		\item[m =   50] 0.9688
		\item[m =  100] 0.9600
		\item[m =  150] 0.9587
		\item[m =  200] 0.9587
		\item[m =  250] 0.9575
		\item[m = 300+] 0.9563
	\end{description}
	On peut constater que malgré la réduction de composantes principales, le 1-NN reste très fiable. On observe même une précision presque équivalente pour un m = 20.
	On peut voir qu'à partir de 300, le nombre de composantes n'a plus d'importance et que la précision reste stable. On peut également voir que l'ajout de composantes n'aura pour effet que d'ajouter du bruit puisque le pic se situe autour de m = 20 et qu'il ne fait que descendre par la suite. Cela démontre l'intérêt de PCA qui permet de conserver les composantes cruciales qui permettent au k-NN de bien identifier le label correspondant à l'image.	
\end{document}